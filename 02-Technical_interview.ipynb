{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Tech Interview\n",
    "\n",
    "\n",
    "## What you will learn in this course üßêüßê\n",
    "\n",
    "Now let's move on to the difficult part: the interview. This course is designed to give you tips and prepare you for the most common questions you might have as a Data Scientist. You will learn :\n",
    "\n",
    "*  To communicate your skills well\n",
    "*  Common questions in Data Science interviews\n",
    "*  How to answer these questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# What's the hiring process in Data Science? üßë‚Äçü§ù‚Äçüßëüßë‚Äçü§ù‚Äçüßë\n",
    "\n",
    "The recruitment process can vary from company to company, most often you will go through the following steps :\n",
    "\n",
    "\n",
    "## Human Resources Interview üòáüòá\n",
    "\n",
    "The HR interview is used to get to know your personality and your motivation to work for the company you are targeting. During this interview, you will be asked questions such as:\n",
    "\n",
    "* Why do you want to work for our company?\n",
    "* What are your qualities and flaws?\n",
    "* Why are you qualified for the job you are applying?\n",
    "\n",
    "This interview should be prepared like a classic interview. You won't have any technical questions during this step. \n",
    "\n",
    "## Technical interview üí´üí´\n",
    "\n",
    "During the technical interview, you will have to demonstrate your knowledge in Data Science. So it's time to study and review all the content of your fullstack program! This interview will most often be led by a member of the Data Science team.\n",
    "\n",
    "## Coding Challenge üë©‚Äçüíªüë©‚Äçüíª\n",
    "\n",
    "In addition to the technical interview, some companies require you to solve a coding challenge. Most of the time, you won't be limited in terms of tool you can use. So don't hesitate to go on the internet and search in Stack Overflow or write on Jedha's Slack channels!\n",
    "\n",
    "## Final interview üöÄüöÄ\n",
    "\n",
    "Finally, you will have a final interview again with a manager which is most likely going to be your N+1. This interview is meant to validate that you fit with the company. Like the HR interview, the purpose here is to test your personality.\n",
    "\n",
    "### As a side note ‚úã‚úã\n",
    "\n",
    "Depending on the company, you won't necessarely follow the exact above steps. Sometimes, you may have only two interviews without a coding challenge, or you may have only one interview with a coding challenge.  \n",
    "\n",
    "Still, you should prepare for each of these steps, because you will definitely encounter at least a Coding Challenge and an HR interview. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions to expect and prepare üëåüëå\n",
    "\n",
    "Let's focus here on the technical interview. We have prepared the most common questions and answers you will have during the technical interview. We can group them into two categories:\n",
    "\n",
    "1. Math questions about statistics and probability üßÆ\n",
    "2. Coding questions üíª\n",
    "\n",
    "## Math / Data Science questions  üßÆ üßÆ\n",
    "\n",
    "### What to expect ü§î\n",
    "\n",
    "This category is intended to test your knowledge in Machine Learning, Statistics and Probabilities. During this step of the interview, we don't expect you to demonstrate the normal law of probability but we do want you to show that you have a strong level of knowledge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General questions / project management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe briefly one project involving data analysis / machine learning that you worked on. What did you achieve ? What were the main problems you encountered ? How did you overcome these difficulties ?\n",
    "\n",
    "* You can describe a project that you made during the *fullstack* or a personal project !\n",
    "* Be proud of what you did, don't minimize your achievements !\n",
    "\n",
    "Example of \"problems\" :\n",
    "* \"I had to contribute to a code that was written in a language I never used before. I learnt by myself from tutorials I found on the internet and asked for help on stackoverflow.com\"\n",
    "* \"Some important data was missing, so I had to develop a scraping code to recover it from public websites\"\n",
    "* \"The data was very unclean and quite inconsistent. I used EDA and statistics to quantify the problem : rate of missing values, presence of unexplained zeros... And then I suggested to my client that we plan a meeting with the person who created the dataset to identify the cause of the problem.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You're working for Netflix and you're responsible for the recommender system algorithms that are in production. For a mysterious reason, the algorithms start to suggest some pornographic/violent content to children ! What do you  do ?\n",
    "\n",
    "This kind of questions is made to see if you're able to prioritize the actions to be taken in case of unexpected problem.\n",
    "* \"Firstly, I ask my colleagues to unplug all the recommender systems so that it stops immediately. Then, I analyze the code and the data to understand what happened.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the main differences between AI, Machine Learning, and Deep Learning?\n",
    "\n",
    "* AI stands for Artificial Intelligence. It is a technique which enables machines to mimic human behavior.\n",
    "* Machine Learning is a subset of AI which uses statistical methods to enable machines to improve with experiences.\n",
    "* Deep learning is a part of Machine learning, which makes the computation of multi-layer neural networks feasible. It takes advantage of neural networks to simulate human-like decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Facebook: _There's a game where you are given two fair six-sided dice and asked to roll. If the sum of the values on the dice equals seven, then you win \\$21. However, you must pay \\$5 to play each time you roll both dice._ \n",
    "\n",
    "1. _Do you play this game?_\n",
    "2. _What is the probability of making money from this game?_\n",
    "\n",
    "*   In order to win \\$21, you need to come up with (1,6), (2,5), (3,4), (4,3), (5,2), (6,1) on two dice. There are $6 \\times 6=36$ possible outcomes, so the chance of wining in $6/36 = 1/6$, in which case you actually earn 21-5= \\$16. The chance of loosing is of course 5/6 and you earn -$5. So, your expected earning is:\n",
    "\n",
    "*   $1/6 \\times 16 + 5/6 \\times (-5) = -1.5$\n",
    "\n",
    "*   So, you are expected to loose money in average!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Facebook: _Let's say we have two options for serving ads within a user's newsfeed:_\n",
    "\n",
    "**1. _1 out of every 25 posts will be an ad_**\n",
    "\n",
    "**2. _every post has a 4% chance of being an ad_**\n",
    "\n",
    "**_For each option, what is the expected number of ads shown in 100 new posts? If we go with option 2, what are the chances that a user will be shown only a single ad out of 100 posts? What about no ads at all?_**\n",
    "\n",
    "\n",
    "1. 4% each option\n",
    "\n",
    "2. Most of the answers are wrong for option #2. The expected value for option #2 is NOT 4 ads! you should rather create a probability distribution.\n",
    "\n",
    "    *   P(ads = 0) = 1.69%\n",
    "    *   P(ads = 1) = 7.03%\n",
    "    *   P (ads = 2) = 14.5%\n",
    "    *   P(ads = 3) = 19.73%\n",
    "    *   P (ads = 4) = 19.94%\n",
    "    *   P(ads = 5) = 15.95%\n",
    "\n",
    "\n",
    "Check Binomial distribution : [https://en.wikipedia.org/wiki/Binomial_distribution](https://en.wikipedia.org/wiki/Binomial_distribution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics / Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _How would you detect outliers in a dataset?_\n",
    "\n",
    "* Data points that are not within 2 standard deviation are generally called as outliers. While preparing the training Data set, we can either drop those data points or we can replace them with the average value of column in case of normal distribution or with median in case of skewed distribution or any values based on the case.\n",
    "\n",
    "* You can also create boxplots for each variable of your dataset and consider each data point that is beyond the whiskers can be considered as outliers. This means $(Q1 - 1.5 \\times IQR)$ or $(Q3 + 1.5 \\times IQR)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Facebook: _70% of iOS Facebook users use Instagram, but only 35% of Android Facebook users use Instagram, how would you investigate this discrepancy?_\n",
    "\n",
    "*   Percentages brings confusion here. It is defintely possible that there are 10 times more Android users in the world than iOS users, so you should question this comparison to start with. \n",
    "\n",
    "*   The number of users probably outranks in Android than in iOS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _You are given a dataset that has missing values which spread out along 1 standard deviation from the median. What percentage of data would remain unaffected? Why?_\n",
    "\n",
    "*   This question has enough hints for you to start thinking! Since, the data is spread out along the median, let‚Äôs assume it‚Äôs a normal distribution. We know that in a normal distribution ~68% of the data lies within 1 standard deviation from mean (or mode/median), which leaves ~32% of the data unaffected. Therefore, ~32% of the data should remain unaffected by missing values.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Let‚Äôs say 8 variables out of 50 have missing values in your dataset. How will you deal with them?_\n",
    "\n",
    "1. Assign a unique category to missing values, who knows the missing values might decipher some trend.\n",
    "\n",
    "2. We can remove them blatantly.\n",
    "\n",
    "3. Or, we can sensibly check their distribution with the target variable, and if found any pattern we‚Äôll keep those missing values and assign them a new category while removing others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Global warming led to a decreased number of pirates around the world. Does that mean that decreasing the number of pirates will lead to reduce global warming?_\n",
    "\n",
    "*   After reading this question, you should understand that this is a classic case of ‚Äúcausation and correlation‚Äù. \n",
    "\n",
    "*   Just because global warming led to a decreased number of pirates doesn't mean that the other way around is true. There might be other factors (lurking or confounding variables) influencing this phenomenon.\n",
    "\n",
    "\n",
    "*   Therefore, there might be a correlation between global warming and the number of pirates, but based on this information we can‚Äôt say that pirated died because of a rise of the global average temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL process / Big data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does the acronym \"ETL\" mean ? Explain briefly the different steps\n",
    "\n",
    "\n",
    "\"ETL\" means \"extrat, transform, load\". It corresponds to the general procedure of copying data from one or more sources into a destination system which represents the data differently from the sources.\n",
    "\n",
    "* Extraction involves extracting data from homogeneous or heterogeneous sources\n",
    "* Transformation processes data by data cleaning and transforming them into a proper storage format/structure for the purposes of querying and analysis\n",
    "* Loading describes the insertion of data into the final target database such as an operational database, a data lake or a data warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your manager has asked you to start a new project. He would like to gather some data related to your company's brand from Facebook and Twitter, in order to analyze what people think of the brand. What kind of pipeline would you suggest to collect and store the data ? Cite some technologies that would be useful.\n",
    "\n",
    "Example of ETL process :\n",
    "* Create a `python` script that allows to collect data from Facebook and Twitter. The collection can be done either from their official APIs (`requests` library) or by scraping their platforms (`Scrapy` or `Beautifulsoup`)\n",
    "* Store the data into an intermediate SQL database (`postgreSQL`). The database can be hosted in a cloud platform (`Amazon RDS`).\n",
    "* Create a `python` script that cleans the data (and optionnally process some machine learning tasks like sentiment analysis) and store the results into a datawarehouse that is ready for analysis with simple SQL queries (`Amazon Redshift`). If there's a lot of data to be processed, or if the tasks require a lot of RAM/CPU, we might consider using `Spark` with a `HDFS` cluster to parallelize the tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a lazy evaluation in Spark ?\n",
    "\n",
    "When Spark operates on any dataset, it remembers the instructions. When a transformation such as a `filter()` is called on a RDD/DataFrame, the operation is not performed instantly. Transformations in Spark are not evaluated until you perform an action, which aids in optimizing the overall data processing workflow, known as lazy evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a Parquet file ?\n",
    "\n",
    "Parquet is a columnar format file supported by many other data processing systems. Spark SQL performs both read and write operations with Parquet file and consider it be one of the best big data analytics formats so far. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What file systems does Spark support?\n",
    "\n",
    "The following three file systems are supported by Spark:\n",
    "\n",
    "* Hadoop Distributed File System (HDFS)\n",
    "* Local File system\n",
    "* Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you understand by worker node?\n",
    "\n",
    "* Worker node refers to any node that can run the application code in a cluster. The driver program must listen for and accept incoming connections from its executors and must be network addressable from the worker nodes. \n",
    "* Worker node is basically the slave node. Master node assigns work and worker node actually performs the assigned tasks. Worker nodes process the data stored on the node and report the resources to the master. Based on the resource availability, the master schedule tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Explain machine learning to me like a 5 year old._\n",
    "\n",
    "*   It‚Äôs simple. It‚Äôs just like how babies learn to walk. Every time they fall down, they learn & realize that their legs should be straight and not in a bend position. The next time they fall down, they feel pain. They cry. But, they learn \"not to stand like that again\". In order to avoid that pain, they try harder. To succeed, they even seek support from the door or wall or anything near them, which helps them stand firm.\n",
    "\n",
    "*   This is how a machine works & develops intuition from its environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **From Orange:** _What's the difference between a supervised model and an unsupervised model?_\n",
    "\n",
    "*   In a supervised model you predict a number or a category. \n",
    "*   In an unsupervised model you create segmentations or dimensional reductions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name one classification model and describe it briefly\n",
    "\n",
    "* Logistic regression : for each example, the probability of belonging to the positive class $P(Y=1)$ is represented by a linear combination of the features : $P(Y=1) = \\sum \\alpha_i X_i$. This probability is compared to a fixed threshold $\\lambda$ (the decision threshold). If $P(Y=1) > \\lambda$ then the model will predict $Y = 1$, else $Y = 0$. The training consists in determining the set of coefficients $\\alpha_i$ that will lead to the best possible predictions.\n",
    "\n",
    "* Decision tree : some successive tests are made on the features describing a given example (one test might be, for example : \"$X_1 < 5$ ?\"). The suite of tests can be represented schematically as a tree. After all the tests are done, each example is assigned to a terminal node (called a leaf). Each leaf corresponds to a class ($Y = 1$ or $Y = 0$), allowing to finally assign a class to the example. The training consists in determining the best set of tests to be done to separate efficiently the positive examples ($Y = 1$) from the negative examples ($Y = 0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### _What is overfitting, and how to avoid it?_\n",
    "\n",
    "* **Overfitting is a Machine Learning model that fits too closely to the real data** and therefore does not generalize. In simpler terms, your algorithms is memorizing instead of generalizing. üö® **Always generalize never memorize!** üö®\n",
    "\n",
    "* **Add more data**: If you can, adding more data in your dataset will definitely remove overfitting. That's THE first answer that you should give to anybody who is asking you this question.\n",
    "\n",
    "* **Regularization**: Ridge or LASSO are two technics that are meant to avoid overfitting. Check out our courses on Machine Learning to see how it works! \n",
    "\n",
    "* **Reducing the number of features**: If you reduce the number of features, your model won't have as many variables to train on and therefore won't be able to bother with small details of your dataset. \n",
    "\n",
    "* **Bagging** is also another Machine Learning technic that will increase biais and therefore reduce overfitting. Random Forests algorithms are a good example. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _What do you understand by Bias / Variance trade off?_\n",
    "\n",
    "*   The error emerging from any model can be broken down into three components mathematically. Following are these component :\n",
    "\n",
    "$$Err(x) = (E[\\hat{f}(x)]-f(x))^2+E[\\hat{f}(x)-E[\\hat{f}(x) - E[\\hat{f}(x)]]^2+\\sigma_{e}^2$$\n",
    "\n",
    "$$ Err(x) = Bias^2+Variance + Irreducible Error $$\n",
    "\n",
    "\n",
    "*   Bias error is useful to quantify how much on average are the predicted values far from the actual values. \n",
    "\n",
    "*   A high bias error means we have a under-performing model which keeps on missing important trends. Variance on the other hand quantifies how each prediction are far from each other on same observation. A high variance model will over-fit on your training population and perform badly on any observation beyond training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _You happen to know that your model is suffering from low bias and high variance. Which algorithm should you use to alleviate this? Why?_\n",
    "\n",
    "*   Low bias occurs when a model‚Äôs predicted values are near to actual values. In other words, the model becomes flexible enough to mimic the training data distribution. While it sounds like great achievement, a too high variance model has no generalization capabilities. \n",
    "\n",
    "*   In such situations, we can use bagging algorithm (like random forest) to reduce high variance problem. Bagging algorithms divides a data set into subsets made with repeated randomized sampling. Then, these samples are used to generate a set of models using a single learning algorithm. Later, the model predictions are combined using voting (classification) or averaging (regression).\n",
    "\n",
    "*   Another way to alleviate high variance, you can:\n",
    "\n",
    "    1. Use regularization technics where higher model coefficients get penalized, hence lowering model complexity.\n",
    "\n",
    "    2. Use top `n-features` from a feature-importance graph. Maybe, with all the variable in the dataset, the algorithm is having a hard time finding meaningful signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _I know that a linear regression model is generally evaluated using Adjusted R¬≤ or F value. How would you evaluate a logistic regression model?_\n",
    "\n",
    "\n",
    "*   Since logistic regression is used to predict probabilities, we can use a AUC-ROC curve along with a confusion matrix to determine its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _What do you understand by Type I vs Type II error?_\n",
    "\n",
    "*   Type I error is when the null hypothesis is true but rejected, also known as a ‚ÄòFalse Positive‚Äô. Type II error is when the null hypothesis is false but we accepted, also known as ‚ÄòFalse Negative‚Äô.\n",
    "\n",
    "*   In the context of confusion matrix, we can say Type I error occurs when we classify a value as positive (1) when it is actually negative (0). Type II error occurs when we classify a value as negative (0) when it is actually positive(1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _When does regularization becomes necessary in Machine Learning?_\n",
    "\n",
    "*   Regularization becomes necessary when the model begins to overfit or underfit. It helps reducing model complexity so that the model can become better at predicting (generalizing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _When is Ridge better than Lasso ?_\n",
    "\n",
    "*   In presence of few variables with medium-to-large dataset, use LASSO regression. In presence of many variables with small-to-medium dataset, use Ridge regression.\n",
    "\n",
    "*   Conceptually, LASSO regression (L1) does both feature-selection and parameter shrinkage, whereas Ridge regression only does parameter shrinkage and end up including all the coefficients in the model. \n",
    "\n",
    "*   In presence of correlated variables, ridge regression might be the preferred choice. Also, ridge regression works best in situations where the least square estimates has a higher variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _You are working on a classification problem. For validation purposes, you‚Äôve randomly sampled the training dataset into a train and a validation set. You are confident that your model will work incredibly well on unseen data since your validation accuracy is high. However, you get shocked after getting poor test accuracy. What went wrong?_\n",
    "\n",
    "*   In case of classification problem, we should always use stratified sampling instead of random sampling. A random sampling doesn‚Äôt takes into consideration the proportion of target classes. On the contrary, stratified sampling helps to maintain the distribution of target variables in the resultant distributed samples also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Considering the long list of machine learning algorithm, given a dataset, how do you decide which one to use?_\n",
    "\n",
    "*   You should say, the choice of machine learning algorithm solely depends of the type of data. If you are given a dataset which shows linearity, then linear regression would be the best algorithm to use. If you are given images, audios, then neural network would help you to build a robust model.\n",
    "\n",
    "*   If the data is non linear, then a boosting or bagging algorithm should be the choice. If the business requirement is to build a model which can be deployed, then we‚Äôll use regression or a decision tree model (easy to interpret and explain) instead of black box algorithms like SVM, GBM, etc.\n",
    "\n",
    "*   In short, there is no master algorithm for all situations. We must be scrupulous enough to understand which algorithm to use in which case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Running a binary classification tree algorithm is the easy part. Do you know how does a tree splitting takes place i.e. how does the tree decide which variable to split at the root node and succeeding nodes?_\n",
    "\n",
    "*   A classification tree makes decision based on a Gini Index and Node Entropy. In simple words, the tree algorithm find the best possible feature which can divide the dataset into purest possible children nodes.\n",
    "\n",
    "*   Gini index measures the impurity of a distribution, the closer to 1 the higher the probability you'll have to misclassify a datapoint within a node. We can calculate Gini as following:\n",
    "\n",
    "1. Calculate Gini for subnodes, using formula sum of probability for success and failure. \n",
    "\n",
    "\n",
    "$$ Gini = \\sum^{Classes}_{i=1} p(i)(1-p(i)) $$\n",
    "\n",
    "where $p$ is the probability of classifiying a datapoint correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _You‚Äôve built a random forest model with 10000 trees. You are thrilled after getting training error as 0.00. But, the validation error is 34.23. What is going on? Haven‚Äôt you trained your model perfectly?_\n",
    "\n",
    "*   The model has overfitted. Training error 0.00 means the classifier has memorized the training data patterns instead of generalizing them. \n",
    "\n",
    "*   Hence, when this classifier was run on unseen data, it couldn‚Äôt find those patterns and returned prediction with higher error. In random forest, it happens when we use a larger number of trees than necessary. To avoid these situations, you should tune the number of trees using a Grid Search and Cross Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _What is the difference between bagging and boosting?_\n",
    "\n",
    "* In bagging techniques, the data set is divided into subsets using sampling with replacement, then a model is trained on each subset. The final prediction is given by voting or averaging result of the models. Bagging is done in parallel. \n",
    "\n",
    "* Boosting, however, is sequential. After each round of prediction, the mis-predicted results get higher weight, so that they gain more attention in the next round of training.\n",
    "\n",
    "* Both bagging and boosting are ensemble learning strategies.\n",
    "\n",
    "* Algorithms that use Bagging: Random Forests \n",
    "\n",
    "* Algorithms that use Boosting: AdaBoost, XGBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Both being tree based algorithm, how is Random Forest different from Gradient Boosting algorithm (GBM)?_\n",
    "\n",
    "*   The fundamental difference is that random forest uses bagging technique to make predictions. GBM uses boosting techniques to make predictions.\n",
    "\n",
    "*   With Bagging, a dataset is divided into `n-samples` using randomized sampling. Then, using a single learning algorithm, a model is built on all samples. Later, the resultant predictions are combined using voting or averaging. Bagging is done in parallel. \n",
    "\n",
    "*   In boosting, after the first round of predictions, the algorithm weighs misclassified predictions higher so that it can be fixed in the succeeding round. This sequential process of giving higher weights to misclassified predictions and will continue until a stopping criterion is reached.\n",
    "\n",
    "*   Random forest improves model accuracy by reducing variance (mainly). The trees grown are uncorrelated to maximize the decrease in variance. On the other hand, GBM improves accuracy by reducing both bias and variance in a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _After spending several hours working on your data, you can now build a Machine Learning model. As a result you build 5 GBM models thinking that a boosting algorithm would do the trick._ _Unfortunately, neither of these models performed better than the current model in place. Therefore, you decided to combine your models. Though, ensembled models are known to return high accuracy, this time you are unfortunate. What did you miss?_\n",
    "\n",
    "*   As we know, ensemble learners are based on the idea of combining weak learners to create strong ones. But these learners provide superior results when the combined models are uncorrelated. \n",
    "\n",
    "*   Since, we have used 5 GBM models and got no accuracy improvement, suggesting that the models are correlated. The problem with correlated models is that they all the models provide the same information.\n",
    "\n",
    "*   For example: If model 1 has classified User1122 as 1. There is a high chance that model 2 and model 3 would have done the same, even if its actual value is 0. That is why ensemble learners are built on the premise of combining weak uncorrelated models to obtain better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Which machine learning model would you use to classify fraudulent transactions on credit cards?_\n",
    "\n",
    "* **Logistic Regression or Random Forests** would be a good fit \n",
    "\n",
    "* **Naive Bayes** would work also for small datasets \n",
    "\n",
    "* **You have to pay attention to imbalanced data** which often the case for fraudulent transactions. This means you will most likely have a lot a non-fraudulent transactions and a very few fraudulent ones. Therefore, you model will give you high accuracy score but still be under-performing when it comes to actually detecting frauds. \n",
    "\n",
    "* **Hypertuning** and balancing a dataset might be a good idea for this kind of usecase. \n",
    "\n",
    "    * [10 technics to deal with imbalanced data](https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/)\n",
    "    * [Balanced Accuracy Score Sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _You are given a training data with 1000 columns and 1 million rows. This is a classification problem. Your manager has asked you to reduce the dimension of this data so that computation is reduced. In addition, your machine has RAM constraints. What would you do? (You are free to make practical assumptions.)_\n",
    "\n",
    "*   Processing a high dimensional data on a limited-RAM machine is a strenuous task, your interviewer is fully aware of that. Here are some methods you can use to tackle such situation:\n",
    "\n",
    "    1. Since we have low RAM, we should close all other applications in our machine, including the web browser, so that most of the memory can be put to use.\n",
    "\n",
    "    2. We can randomly sample the dataset which will create a smaller dataset. Let‚Äôs say, having 1000 variables and 300000 rows.\n",
    "    \n",
    "    3. To reduce dimensionality, we can separate the numerical and categorical variables and remove correlated variables. For numerical variables, we‚Äôll use correlation. For categorical variables, we‚Äôll use chi-square test.\n",
    "\n",
    "    4. We can use [PCA](https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/) and pick the components which can explain the maximum variance in the data set.\n",
    "\n",
    "    5. We can also apply business knowledge to estimate which predictors can impact the target variable. However this is an intuitive approach. Failing to identify useful predictors might result in significant loss of information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _You are given a dataset related to cancer cells detection. You‚Äôve built a classification model and reached an accuracy of 96%. Why shouldn‚Äôt you be happy with your model performance? What can you do about it?_\n",
    "\n",
    "*   If you have worked on enough datasets, you should deduce that cancer cells detection is oftentimes an imbalanced dataset. In an imbalanced dataset, accuracy should not be used as a measure of performance because 96% (as given) might only be predicting \"No Cancer\" class. \n",
    "\n",
    "*   Our class of interest is the minority class (4%) which is the people who actually got diagnosed with cancer. Hence in order to evaluate model performance, we could use Recall (True Positive Rate), Specificity (True Negative Rate) or F1 Score to determine our model's performance. If the minority class performance is found to to be poor, we should take the following steps:\n",
    "\n",
    "    1. Use undersampling, oversampling or SMOTE to make the data balanced.\n",
    "\n",
    "    2. Alter the positive threshold value by doing [probability calibration](https://www.analyticsvidhya.com/blog/2016/07/platt-scaling-isotonic-regression-minimize-logloss-error/) and find an optimal threshold using AUC-ROC curve.\n",
    "\n",
    "    3. Assign larger weights to the minority class.\n",
    "\n",
    "    4. Use anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _After analyzing an ML model, your manager let you know that your regression is suffering from multicollinearity. How would you check if he‚Äôs true? Without losing any information, can you still build a better model?_\n",
    "\n",
    "*   To check multicollinearity, we can create a correlation matrix to identify & remove variables that have a correlation above 75% (the threshold is subjective). \n",
    "\n",
    "*   In addition, we can use calculate [VIF (variance inflation factor)](https://www.investopedia.com/terms/v/variance-inflation-factor.asp#:~:text=Variance%20inflation%20factor%20(VIF)%20is,only%20that%20single%20independent%20variable.) to check the presence of multicollinearity. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity. \n",
    "\n",
    "*   You can also use tolerance as an indicator of multicollinearity.\n",
    "\n",
    "*   Removing correlated variables might lead to loss of information though. In order to keep the information contained in the variables you removed, you can use penalized regression models like Ridge or LASSO regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Working on a dataset how do you select the most important variables? Explain your methods._\n",
    "\n",
    "1. Remove the correlated variables prior to selecting important variables\n",
    "\n",
    "2. Use linear regression and select variables based on `p-values`\n",
    "\n",
    "3. Use Forward Selection, Backward Selection, Stepwise Selection\n",
    "\n",
    "4. Use Random Forest, Xgboost or any other Machine Learning algorithm where you can easily plot a feature-importance chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _We know that one hot encoding increases the dimensionality of a dataset. But, label encoding doesn‚Äôt. How?_\n",
    "\n",
    "*   Don‚Äôt get baffled by this question. It‚Äôs a simple question asking the difference between the two.\n",
    "\n",
    "*   Using one hot encoding, the dimensionality (a.k.a features) in a dataset get increased because it creates a new variable for each category. \n",
    "\n",
    "*   For example: let‚Äôs say we have a variable ‚Äòcolor‚Äô. The variable has 3 categories: Red, Blue and Green. One hot encoding ‚Äòcolor‚Äô variable will generate three new variables as \n",
    "    *   Red \n",
    "    *   Blue\n",
    "    *   Green\n",
    "    *   All containing 0 and 1 value\n",
    "\n",
    "* On the other hand, in label encoding, the categories are simply encoded as numbers, so no new variable is created. Label encoding is majorly used for binary variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Do you suggest that treating a categorical variable as continuous variable would result in a better predictive model?_\n",
    "\n",
    "*   For better predictions, categorical variable can be considered as a continuous variable only when the variable is ordinal in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe the k-means algorithm\n",
    "\n",
    "K-means is a clustering algorithm. It works as follows : \n",
    "\n",
    "* Specify number of clusters K.\n",
    "* Initialize centroids by first shuffling the dataset and then randomly selecting K data points for the centroids without replacement.\n",
    "* Keep iterating until there is no change to the centroids. i.e assignment of data points to clusters isn‚Äôt changing :\n",
    "    * Compute the sum of the squared distance between data points and all centroids.\n",
    "    * Assign each data point to the closest cluster (centroid).\n",
    "    * Compute the centroids for the clusters by taking the average of the all data points that belong to each cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _You are given a dataset that contains many variables. Some of them are highly correlated. Your manager has asked you to run PCA. Would you remove correlated variables first? Why?_\n",
    "\n",
    "*   Chances are, you might be tempted to say no. But that would be incorrect. Discarding correlated variables have a substantial effect on PCA because, in presence of correlated variables, the variance explained by a particular component gets inflated.\n",
    "\n",
    "*   For example: You have 3 variables in a dataset. 2 of them are correlated. If you run PCA on this dataset, the first principal component would show twice as much variance as it would with uncorrelated variables. \n",
    "\n",
    "*   Also adding correlated variables lets PCA put more importance on those variables, which is misleading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Is rotation necessary in PCA? If yes, why? What will happen if you don‚Äôt rotate the components?_\n",
    "\n",
    "*   Yes, rotation (orthogonal) is necessary because it maximizes the difference between variances captured by the component. This makes the components easier to interpret. Not to forget, that‚Äôs the purpose of doing PCA where we aim to select fewer components (than features) which can explain the maximum variance in the dataset. By doing rotation, the relative location of the components doesn‚Äôt change, it only changes the actual coordinates of the points.\n",
    "\n",
    "*   If we don‚Äôt rotate the components, the effect of PCA will decrease and we‚Äôll have to select more number of components to explain variance in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeseries analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is timeseries analysis ?\n",
    "\n",
    "A time series is defined as quantity that is measured sequentially in time over some interval. Time series analysis attempts to understand the past and predict the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _You are working on a time series dataset. You manager has asked you to build a high accuracy model. You start with a decision tree algorithm. Since you know it works fairly well on all kinds of data. Later, you tried a time series regression model and got higher accuracy than decision tree model. Is this possible? Why?_\n",
    "\n",
    "*   Time series data is known to be usually linear. On the other hand, a decision tree algorithm is known to work best on detecting non‚Äìlinear interactions. \n",
    "\n",
    "*   The reason why decision tree failed to provide robust predictions is that it is unable to build a strong linear relationship as well as a regression model did. That is why a linear regression model can provide robust predictions because it meets all the [linearity assumptions](https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _What cross validation technic would you use on time series dataset?_\n",
    "\n",
    "\n",
    "*   None\n",
    "\n",
    "*   In time series problems, k fold can be troublesome because there might be some pattern in year 4 or 5 that are not in year 3. \n",
    "\n",
    "*   Instead, we can use forward chaining strategy with 5 fold as shown below:\n",
    "    *   fold 1 : training [1], test [2]\n",
    "    *   fold 2 : training [1 2], test [3]\n",
    "    *   fold 3 : training [1 2 3], test [4]\n",
    "    *   fold 4 : training [1 2 3 4], test [5]\n",
    "    *   fold 5 : training [1 2 3 4 5], test [6]\n",
    "\n",
    "    where 1,2,3,4,5,6 represents ‚Äúyear‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe the moving average model in two simple sentences\n",
    "\n",
    "In time series analysis, the moving-average (MA) model is a common approach for modeling univariate time series. The moving-average model specifies that the output variable depends linearly on the current and various past values of a stochastic (imperfectly predictable) term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does autocorrelation measures ?\n",
    "\n",
    "The autocorrelation measures the linear dependence between two points on the same series observed at different times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is NLTK ?\n",
    "\n",
    "NLTK is a Python library, which stands for Natural Language Toolkit. We use NLTK to process data in human spoken languages. NLTK allows us to apply techniques such as parsing, tokenization, lemmatization, stemming, and more to understand natural languages. It helps in categorizing text, parsing linguistic structure, analyzing documents, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are stop words ?\n",
    "\n",
    "Stop words are said to be useless data for a NLP engine. Words such as articles, prepositions, etc. are considered as stop words. There are stop words such as \"was\", \"were\", \"is\", \"am\", \"the\", \"a\", \"an\", \"how\", \"why\", and many more. In Natural Language Processing, we eliminate the stop words to understand and analyze the meaning of a sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does \"TF-IDF\" mean ?\n",
    "\n",
    "TF-IDF means \"Term Frequency-Inverse Document Frequency\". It is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. The TF-IDF is the product of two statistics : term frequency and inverse document frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are unigrams, bigrams, trigrams, and n-grams ?\n",
    "\n",
    "When we parse a sentence one word at a time, then it is called a unigram. The sentence parsed two words at a time is a bigram. When the sentence is parsed three words at a time, then it is a trigram. Similarly, n-gram refers to the parsing of n words at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the steps involved in solving an NLP problem with machine learning techniques ?\n",
    "\n",
    "Below are the steps involved in solving an NLP problem:\n",
    "\n",
    "* Gather the text from the available dataset or by web scraping\n",
    "* Apply stemming or lemmatization for text cleaning\n",
    "* Remove stop words\n",
    "* Encore the documents, for example with TF-IDF (or embed them using word2vec)\n",
    "* Train the model using neural networks or other Machine Learning techniques\n",
    "* Evaluate the model‚Äôs performance\n",
    "* Make appropriate changes in the model\n",
    "* Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cite some applications of Deep Learning\n",
    "\n",
    "* Computer vision\n",
    "* Natural language processing and pattern recognition\n",
    "* Image recognition and processing\n",
    "* Machine translation\n",
    "* Sentiment analysis\n",
    "* Question Answering system\n",
    "* Object Classification and Detection\n",
    "* Automatic Handwriting Generation\n",
    "* Automatic Text Generation\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a Neural Network ?\n",
    "\n",
    "Neural Networks replicate the way humans learn, inspired by how the neurons in our brains fire, only much simpler.\n",
    "The most common Neural Networks consist of three network layers:\n",
    "\n",
    "* An input layer\n",
    "* A hidden layer (this is the most important layer where feature extraction takes place, and adjustments are made to train faster and function better)\n",
    "* An output layer that performs the final prediction\n",
    "\n",
    "Each layer contains neurons called ‚Äúnodes‚Äù or \"units\", performing various operations and passing the result to the next layer. Neural Networks are used in deep learning algorithms like CNN, RNN, GAN, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the role of activation functions in a Neural Network ?\n",
    "\n",
    "At the most basic level, an activation function decides whether a neuron should be fired or not. It accepts the weighted sum of the inputs and bias as input to any activation function. Step function, Sigmoid, ReLU, Tanh, and Softmax are examples of activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens if we don't use any activation function in a Neural Network ?\n",
    "\n",
    "The main aim of this question is to understand why we need activation functions in a neural network. You can start off by giving a simple explanation of how neural networks are built:\n",
    "\n",
    "* Step 1: Calculate the sum of all the inputs ($X$) according to their weights and include the bias term:\n",
    "$$\n",
    "Z = (weights * X) + bias\n",
    "$$\n",
    "\n",
    "* Step 2: Apply an activation function to calculate the expected output:\n",
    "\n",
    "$$\n",
    "Y = Activation(Z)\n",
    "$$\n",
    "\n",
    "Steps 1 and 2 are performed at each layer. If you recollect, this is nothing but forward propagation! Now, what if there is no activation function?\n",
    "\n",
    "Our equation for $Y$ essentially becomes:\n",
    "\n",
    "$$\n",
    "Y = Z = (weights * X) + bias\n",
    "$$\n",
    "\n",
    "Wait ‚Äì isn‚Äôt this just a simple linear equation? Yes ‚Äì and that is why we need activation functions. A linear equation will not be able to capture the complex patterns in the data ‚Äì this is even more evident in the case of deep learning problems.\n",
    "\n",
    "In order to capture non-linear relationships, we use activation functions, and that is why a neural network without an activation function is just a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does forward propagation and backpropagation work in deep learning?\n",
    "\n",
    "* **Forward propagation**: The inputs are provided with weights to the hidden layer. At each hidden layer, we calculate the output of the activation at each node and this further propagates to the next layer till the final output layer is reached. Since we start from the inputs to the final output layer, we move forward and it is called forward propagation\n",
    "* **Backpropagation**: We minimize the cost function by its understanding of how it changes with changing the weights and biases in a neural network. This change is obtained by calculating the gradient at each hidden layer (and using the chain rule). Since we start from the final cost function and go back each hidden layer, we move backward and thus it is called backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  What Are the Different Layers on CNN?\n",
    "\n",
    "* Convolutional Layer -  the layer that performs a convolutional operation, creating several smaller picture windows to go over the data.\n",
    "* ReLU Layer - it brings non-linearity to the network and converts all the negative pixels to zero. The output is a rectified feature map.\n",
    "* Pooling Layer - pooling is a down-sampling operation that reduces the dimensionality of the feature map.\n",
    "* Fully Connected Layer - this layer performs the prediction (for example, it classifies the image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment / Cloud computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Describe the different cloud service models\n",
    "\n",
    "There are predominantly three models of cloud service. Each come with their own sets of advantages and are at variance with each other with regards to one or the other features. \n",
    "\n",
    "* **IaaS** - Infrastructure as a Service (IaaS) consists of highly automated compute resources. Businesses can avail of on-demand hardware resources through IaaS without having to make any upfront hardware purchase. IaaS is highly scalable and can assist in quickly accessing and monitoring computers, database storage, and other networking services\n",
    "* **PaaS** -Platform as a Service (PaaS) is helpful in customizing applications that require cloud components. PaaS helps in streamlining the workflow in the situations which involve more than one developer. While developers can manage the applications, businesses get to use the network and storage. \n",
    "* **SaaS** - Software as a Service (SaaS) refers to the service model where applications are delivered to the user using cloud platforms, and the third party can then manage the applications. They are incredibly convenient to use since they do not require any additional installations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name at least two cloud computing platforms\n",
    "\n",
    "* Amazon Web Service\n",
    "* Google Cloud Platform\n",
    "* Microsoft Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cite at least two services available on AWS\n",
    "\n",
    "* **Amazon S3** : Simple Storage Service (bucket files)\n",
    "* **Amazon RDS** : Relationnal Database such as PostgreSQL\n",
    "* **Amazon Redshift** : Datawarehouse\n",
    "* **Amazon Sagemaker** : Deploy machine learning models in the cloud "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  What is Git?\n",
    "Git is a version control system for tracking changes in computer files and is used to help coordinate work among several people on a project while tracking progress over time. In other words, it‚Äôs a tool that facilitates source code management in software development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's the difference between Git and GitHub ?\n",
    "\n",
    "* Git is a version control system for tracking changes in computer files. The main point of Git is to manage projects, or a set of them when changes are made over time. It helps to track progress over time and coordinate work among several people on a project.\n",
    "* GitHub is a Git repository hosting service that provides a web-based graphical interface. GitHub helps every team member to work together on the project from anywhere, making collaboration easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name a few git commands with their functions\n",
    "\n",
    "* Git config - Configure the username and email address\n",
    "* Git add - Add one or more files to the staging area\n",
    "* Git commit - Commit changes to head but not to the remote repository\n",
    "* Git push - Upload committed changes to the remote repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mention at least two HTTP methods supported by REST ?\n",
    "\n",
    "* **GET**: It requests a resource at the request URL. It should not contain a request body as it will be discarded. Maybe it can be cached locally or on the server\n",
    "* **POST**: It submits information to the service for processing; it should typically return the modified or new resource\n",
    "* **PUT**: At the request URL it update the resource\n",
    "* **DELETE**: At the request URL it removes the resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Docker?\n",
    "\n",
    "Docker is an open-source lightweight containerization technology. It has gained widespread popularity in the cloud and application packaging world. It allows you to automate the deployment of applications in lightweight and portable containers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a Docker image?\n",
    "\n",
    "The Docker image helps to create Docker containers. You can create the Docker image with the build command. Due to this, it creates a container that starts when it begins to run. Every docker images are stored in the Docker registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a DBMS?\n",
    "\n",
    "A Database Management System (DBMS) is a program that controls creation, maintenance and use of a database. A DBMS can be termed as File Manager that manages data in a database rather than saving it in file systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enlist the advantages of using a DBMS\n",
    "\n",
    "The advantages of DBMS include:\n",
    "\n",
    "* Data is stored in a structured way and hence redundancy is controlled.\n",
    "* Validates the data entered and provide restrictions on unauthorized access to the database.\n",
    "* Provides backup and recovery of the data when required.\n",
    "* It provides multiple user interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a Datawarehouse ?\n",
    "\n",
    "Datawarehouse is a central repository of data from multiple sources of information. Those data are consolidated, transformed and made available for the mining and online processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is SQL?\n",
    "\n",
    "SQL stands for Structured Query Language , and it is used to communicate with a Relational Database. This is a standard language used to perform tasks such as retrieval, updation, insertion and deletion of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are tables and Fields?\n",
    "\n",
    "A table is a set of data that are organized in a model with Columns and Rows. Columns can be categorized as vertical, and Rows are horizontal. A table has specified number of column called fields but can have any number of rows which is called record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a primary key?\n",
    "\n",
    "A primary key is a combination of fields which uniquely specify a row. This is a special kind of unique key, and it has implicit NOT NULL constraint. It means, Primary key values cannot be NULL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the different types of join and explain each?\n",
    "\n",
    "There are various types of join which can be used to retrieve data and it depends on the relationship between tables.\n",
    "\n",
    "* **Inner Join** : Inner join return rows when there is at least one match of rows between the tables.\n",
    "* **Left Join** : Left join return rows which are common between the tables and all rows of Left hand side table. Simply, it returns all the rows from Left hand side table even though there are no matches in the Right hand side table.\n",
    "* **Right Join** : Right join return rows which are common between the tables and all rows of Right hand side table. Simply, it returns all the rows from the right hand side table even though there are no matches in the left hand side table.\n",
    "* **Full Join** : Full join return rows when there are matching rows in any one of the tables. This means, it returns all the rows from the left hand side table and all the rows from the right hand side table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In SQL, how to select unique records from a table ?\n",
    "\n",
    "Select unique records from a table by using DISTINCT keyword.\n",
    "\n",
    "Example :\n",
    "\n",
    "```\n",
    "Select DISTINCT StudentID, StudentName from Student\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which operator is used in a SQL query for pattern matching ?\n",
    "\n",
    "LIKE operator is used for pattern matching. \n",
    "\n",
    "Example :\n",
    "\n",
    "```\n",
    "Select * from Student where studentname like 'a%'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More questions ü§ìü§ì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Why is naive Bayes so ‚Äònaive‚Äô ?_\n",
    "\n",
    "*   Naive Bayes is so ‚Äònaive‚Äô because it assumes that all of the features in a dataset are equally important and independent. As we know, these assumption are rarely true in real world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _How kNN is different from K-Means clustering?_\n",
    "\n",
    "*   Don‚Äôt be misled by the ‚Äòk‚Äô in both names. You should know that the fundamental difference between both these algorithms is that kmeans is unsupervised when kNN is supervised. kmeans is a clustering algorithm. kNN is a classification (or regression) algorithm.\n",
    "\n",
    " *  K-Means algorithm partitions a data set into clusters, such that a cluster formed is homogeneous and the points in each cluster are close to each other. The algorithm tries to maintain enough separability between these clusters. Due to unsupervised nature, the clusters have no labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _What is the difference between covariance and correlation?_\n",
    "\n",
    "*   Correlation is a standardized form of covariance.\n",
    "\n",
    "*   Covariances are difficult to compare. For example: if we calculate the covariances of salary ($) and age (years), we‚Äôll get different covariances which can‚Äôt be compared because of having unequal scales. \n",
    "\n",
    "*   To cope with such situation, we calculate correlation to get a value between -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Is it possible to capture the correlation between continuous and categorical variable? If yes, how?_\n",
    "\n",
    "*   Yes, we can use ANCOVA (analysis of covariance) technics to capture the association between continuous and categorical variables. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _You have been asked to evaluate a regression model based on R¬≤, adjusted R¬≤ and tolerance. What will be your criteria?_\n",
    "\n",
    "*   Tolerance (1 / VIF) is used as an indicator of multicollinearity. It is an indicator of the variance percentage in a predictor which cannot be accounted by other predictors. Large values of tolerance is desirable.\n",
    "\n",
    "*   We will consider adjusted R¬≤ as opposed to R¬≤ to evaluate model fit because R¬≤ increases regardless of improvement in prediction accuracy as we add more variables. \n",
    "\n",
    "*   However, adjusted R¬≤ would only increase if an additional variable improves the accuracy of model, otherwise it stays the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _You‚Äôve got a dataset that has $p$ variables > $n$ observations. Why is OLS a bad option to work with? Which technics would be best to use?_\n",
    "\n",
    "*   In such a high dimensional dataset, you can‚Äôt use classic regression technics. When $p > n$, we can no longer calculate a unique least square coefficient because the variances are becoming infinite, so OLS cannot be used at all.\n",
    "\n",
    "*   To cope with this situation, you can use Ridge or LASSO which can shrink the coefficients to reduce variance. More precisely, Ridge regressions work best in situations where the least square estimates have higher variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _'People who bought this, also bought‚Ä¶' recommendations seen on amazon is a result of which algorithm?_\n",
    "\n",
    "*   The basic idea for this kind of recommendation engine comes from collaborative filtering.\n",
    "\n",
    "*   Collaborative Filtering algorithm considers ‚ÄúUser Behavior‚Äù for recommending items. They learn from other users' behavior and items like transaction history, ratings, selection and purchase information. \n",
    "\n",
    "*   Other users behavior and preferences over a given item are used as recommandation to new users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are NoSQL databases? What are the different types of NoSQL databases?\n",
    "\n",
    "A NoSQL database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases (like PostgreSQL, Oracle, etc.).\n",
    "\n",
    "Types of NoSQL databases:\n",
    "\n",
    "* Document Oriented\n",
    "* Key Value \n",
    "* Graph\n",
    "* Column Oriented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Questions üíªüíª\n",
    "\n",
    "\n",
    "### What to expect üôÉüôÉ\n",
    "\n",
    "Here we're trying to find out how good your coding skills are. Hiring managers will want to see that you master Python or SQL efficiently.\n",
    "\n",
    "Elegant code will always be rewarded, but it won't be a deal-breaker if you provide an imperfect answer.\n",
    "\n",
    "\n",
    "### Exemples\n",
    "\n",
    "#### From Facebook: _Given a list A of objects and another list B which is identical to A except that one element is removed, find that removed element._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function if listA and listB are filled with numbers\n",
    "def rem_elem_num(listA, listB):\n",
    "    sumA = 0\n",
    "    sumB = 0\n",
    "    for i in listA:\n",
    "        sumA += i\n",
    "    for j in listB:\n",
    "        sumB += j\n",
    "    return sumA-sumB \n",
    "\n",
    "###### OR ##### \n",
    "\n",
    "## This function if listA or listB are filled with any type of data\n",
    "def rem_elem(listA, listB):\n",
    "    dictB = {}\n",
    "    for j in listB:\n",
    "        dictB[j] = None\n",
    "    for i in listA:\n",
    "        if i not in dictB:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _How do you verify that an element is in an ordered list of integers? What is the complexity of the algorithm?_\n",
    "\n",
    "*   By dichotomy or recursion. We select the integer in the middle of the list, compare it to our element. If it's bigger, we run the recursion on the first part of the list, if not, on the bigger one. This is done until our list contains more than one element or we have a tie.\n",
    "\n",
    "*  The complexity is in 2log(n) since, with this method, we have 2^n steps if n is the size of the list.\n",
    "\n",
    "‚Üí More info on [https://emilypython.wordpress.com/2018/04/18/resoudre-des-equations-par-dichotomie-avec-python/](https://emilypython.wordpress.com/2018/04/18/resoudre-des-equations-par-dichotomie-avec-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _In how many years were there will be more dead people profiles than alive people profiles on Facebook._\n",
    "\n",
    "\n",
    "1. Set variables: `n` the number of years\n",
    "\n",
    "2. Make assumptions: 1 billion user as of right now, 140 million people born every year, 60 millions people die every year, half of the world population has access to the Internet... Use these to get a reasonable estimation of how many new profiles are created and how many people with an FB profile die.\n",
    "\n",
    "3. Set the equation and solve it\n",
    "\n",
    "*   Its easier if you pick numbers that are easy to do mental maths with. Also every time you stop or hesitate, the interviewers will pressure you, don't let that distract you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Disorienting Questions üôâüôâ\n",
    "\n",
    "### What to expect ü§îü§î\n",
    "\n",
    "It's less common for you to have these kinds of questions, but it's always good to be prepared. The purpose of these questions is not to get an exact answer but to see your logic.\n",
    "\n",
    "Here, you **have to talk**. So ask questions to the person in front of you, make assumptions and move forward.\n",
    "\n",
    "#### Examples\n",
    "\n",
    "#### _How many golf balls can you fill a school bus with?_\n",
    "\n",
    "*   Here we can make an assumption about the volume of a golf ball and the volume of a school bus and then divide one by the other to get an approximate number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Final Notes \n",
    "\n",
    "\n",
    "### Talk ü¶ú\n",
    "\n",
    "Think out loud. It's a difficult exercise, but it also shows your ability to work with other technical profiles!\n",
    "\n",
    "\n",
    "### Ask questions üôã‚Äç‚ôÇÔ∏è\n",
    "\n",
    "If a technical issue is unclear, there is a good chance that it was done on purpose. So the jury expects you to ask questions. Their purpose is to see how well you can solve a problem.\n",
    "\n",
    "\n",
    "### Practice writing on paper üñäÔ∏è\n",
    "\n",
    "You may have a whiteboard exercise where interviewer will ask you to write code by hand instead of on a computer. \n",
    "\n",
    "Although this practice is increasingly being questioned, it is quite possible that you will go through this kind of process. So it's good to have practiced it a little bit before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More coding challenges \n",
    "\n",
    "Hacker Rank:  https://www.hackerrank.com\n",
    "\n",
    "Coding Games :  https://www.codingame.com/start\n",
    "\n",
    "Test dome: https://app.testdome.com/t?GeneratorId=65\n",
    "\n",
    "StrataScratch : https://platform.stratascratch.com/edu-content-editor?id=9616&python= (be careful : there's a free trial but also a version where you'll have to pay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Ressources üìöüìö\n",
    "\n",
    "*   Data Science Prep - [https://bit.ly/231314](https://datascienceprep.com/)\n",
    "\n",
    "*   5 tips to get a data science job - [http://bit.ly/3012](https://www.youtube.com/watch?v=MfP-P8EHGBo)\n",
    "\n",
    "*   Draw me un Data Scientist - [http://bit.ly/ai3iS](https://www.youtube.com/watch?v=iydqaa8NuzE)\n",
    "\n",
    "*   Company interview questions -[ http://bit.ly/ijde8](https://www.glassdoor.com/index.htm)\n",
    "\n",
    "*   40 Interview questions asked at Startups in Machine Learning - [http://bit.ly/jfes9](https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/)\n",
    "\n",
    "*   12 Difficult questions in tech interview - [http://bit.ly/jdekj8](https://www.businessinsider.fr/questions-difficiles-posees-en-entretien-entreprises-tech#pourquoi-je-ne-vous-embaucherai-pas)\n",
    "\n",
    "*   Data Scientist a recruitment process tailored - [http://bit.ly/djeakj92](https://dataanalyticspost.com/data-scientists-un-process-de-recrutement-sur-mesure/)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
